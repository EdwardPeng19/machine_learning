{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-15T12:14:37.042334Z",
     "start_time": "2018-10-15T12:14:35.890629Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import jieba\n",
    "import gensim\n",
    "from gensim.models import Word2Vec, FastText\n",
    "import re\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "from m import f1_for_car, BOW, BasicModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-15T12:14:39.131689Z",
     "start_time": "2018-10-15T12:14:39.075309Z"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test_public.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-15T13:04:53.879024Z",
     "start_time": "2018-10-15T13:04:53.863970Z"
    }
   },
   "outputs": [],
   "source": [
    "subj_lst = list(filter(lambda x : x is not np.nan, list(set(data.subject))))\n",
    "subj_lst_dic = {value:key for key, value in enumerate(subj_lst)}\n",
    "data['subject_1'] = data['subject'].apply(lambda x : subj_lst_dic.get(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['subject_1'] = data['subject'] + data['sentiment_value'].astype(str)\n",
    "subj_lst = list(filter(lambda x : x is not np.nan, list(set(data.subject_1))))\n",
    "subj_lst_dic = {value:key for key, value in enumerate(subj_lst)}\n",
    "data['subject_1'] = data['subject_1'].apply(lambda x : subj_lst_dic.get(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 将训练数据分成训练集和验证集(根据主题进行分层抽样)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-15T12:14:41.381211Z",
     "start_time": "2018-10-15T12:14:41.305196Z"
    }
   },
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=5,shuffle=True)\n",
    "for train_idx, val_idx in skf.split(data, data.subject):\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-15T13:05:18.987331Z",
     "start_time": "2018-10-15T13:05:18.979641Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train = data.iloc[train_idx]\n",
    "df_val = data.iloc[val_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-15T12:14:45.837140Z",
     "start_time": "2018-10-15T12:14:45.826930Z"
    }
   },
   "outputs": [],
   "source": [
    "subj_dict = {'价格':['价格','性价比','低价','降价','优惠','便宜','划算','不菲','实惠','贵','价差','单价','合算','合理','高昂','有钱任性','保值率','费用','同价位','评估价','最高配','最低配','前（钱）紧','8万'],\\\n",
    "            '油耗':['油耗','高速','市区','公里','废油','不见得省','省油','个油','节油','机油','油号','费油','不省什么油'],\\\n",
    "            '配置':['配置','导航','视野','倒车雷达','倒车影像','中控','后视镜','自动泊车','摄像头','前雷达','车载','音质','背光','简配','落锁','出风口'],\\\n",
    "            '内饰':['内饰','氛围','单调','寒酸','用料','细致','设计感','异味','做工','简陋','粗糙','档次','不够整','劣质材料','防火材料'],\\\n",
    "            '操控':['操控','控制','偏硬','不费劲','迟钝','底盘','操纵','减震','方向盘','尾排','加强件','刹车','灵活','韧性','漂移','手感差','变速箱','平衡性'],\\\n",
    "            '空间':['空间','视野','舒服','容量','显小','钻进去','宽敞','宽大','轴距','车体'],\\\n",
    "            '外观':['外观','杀马特','大气','前脸','外形','变色','漆面','油漆','车漆','眼缘','尾灯','帅气','镀铬','镀络','颜值','挺炫','屁股','新潮','里外不一','好看','颜色','寒冰银','蓝色','黑色','不耐脏','银色','红色','蓝棕','黄贴'],\\\n",
    "            '动力':['动力','驱动','发动机','机油','散热','四驱','强劲','变速箱','飙车','爆缸','排量','尾排','爬坡','油门踩到底','怕烧机油'],\\\n",
    "            '安全性':['安全','刹车','手刹','追尾','气囊','加速','扎实','防爆胎','被盗','防盗','失去抓地力'],\\\n",
    "            '舒适性':['舒适','隔音','舒服','噪音','异响','吵','静音','风噪','都会响','出风口','安静','空调','气门','颈椎','累','制冷','恒温','声音','抖','座椅','视野','宽大','晕车','减震','腰疼','卡顿','坐姿','颠簸','气味','滴水','后备箱响']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-15T12:14:46.998310Z",
     "start_time": "2018-10-15T12:14:46.688018Z"
    }
   },
   "outputs": [],
   "source": [
    "cont_id_dict = {}\n",
    "for key, value in subj_dict.items():\n",
    "    cont_id_dict[key] = []\n",
    "    for val in value:\n",
    "        cont_id_dict[key] += df_val[df_val.content.apply(lambda x : val in x)].content_id.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-15T12:14:48.014763Z",
     "start_time": "2018-10-15T12:14:48.008148Z"
    }
   },
   "outputs": [],
   "source": [
    "cont_id_dict2 = {}\n",
    "for key, value in cont_id_dict.items():\n",
    "    for val in value:\n",
    "        if val in cont_id_dict2:\n",
    "            cont_id_dict2[val].append(key)\n",
    "        else:\n",
    "            cont_id_dict2[val] = [key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-15T12:14:48.581501Z",
     "start_time": "2018-10-15T12:14:48.564192Z"
    }
   },
   "outputs": [],
   "source": [
    "def filter_subj1(x):\n",
    "    tmp = Counter(x)\n",
    "    tmp2 = list(filter(lambda y : y[1]>1, tmp.items()))\n",
    "    if len(tmp2) == 0:\n",
    "        return x\n",
    "    return [y[0] for y in tmp2]\n",
    "\n",
    "# 整成dataframe\n",
    "tmp = pd.Series(cont_id_dict2)\n",
    "tmp2 = pd.DataFrame()\n",
    "tmp2['content_id'] = tmp.index\n",
    "tmp2['subject_rule'] = tmp.tolist()\n",
    "# tmp2['subject'] = tmp2['subject'].apply(filter_subj1)\n",
    "# tmp2['subject_rule'] = tmp2['subject_rule'].apply(lambda x : list(set(x)))\n",
    "tmp2['subject_rule'] = tmp2['subject_rule'].apply(lambda x : list(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-15T12:14:50.555439Z",
     "start_time": "2018-10-15T12:14:50.542882Z"
    }
   },
   "outputs": [],
   "source": [
    "df_val = df_val.merge(tmp2,on='content_id',how='left')\n",
    "df_val.subject_rule.fillna('N',inplace=True)\n",
    "df_val['subject_rule'] = df_val['subject_rule'].apply(lambda x : ','.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-15T12:14:51.138575Z",
     "start_time": "2018-10-15T12:14:51.132916Z"
    }
   },
   "outputs": [],
   "source": [
    "def filter_subj1(x):\n",
    "    x = x.split(',')\n",
    "    tmp = Counter(x)\n",
    "    tmp2 = list(filter(lambda y : y[1]>1, tmp.items()))\n",
    "    if len(tmp2) == 0:\n",
    "        return ','.join(x)\n",
    "    return ','.join([y[0] for y in tmp2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-15T12:14:51.783043Z",
     "start_time": "2018-10-15T12:14:51.757076Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1620"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = 0\n",
    "for i, j in zip(df_val['subject'],df_val['subject_rule'].apply(filter_subj1)):\n",
    "    if i in j:\n",
    "        c += 1\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 试一下用LSTM进行主题分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-15T12:15:10.663832Z",
     "start_time": "2018-10-15T12:15:03.802679Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.976 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "stop_mark = ['，','。','！',',','!','？','?','',' ']\n",
    "def f1(x):\n",
    "    tmp_l = jieba.cut(x)\n",
    "    return [x.strip() for x in tmp_l if x.strip() not in stop_mark]\n",
    "\n",
    "word2vec = Word2Vec(data.content.apply(f1).tolist(),size=300,min_count=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-15T12:15:27.764295Z",
     "start_time": "2018-10-15T12:15:23.434484Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Word Count: 100%|██████████| 9947/9947 [00:00<00:00, 153908.48it/s]\n",
      "Doc To Number: 100%|██████████| 9947/9947 [00:00<00:00, 87790.45it/s]\n"
     ]
    }
   ],
   "source": [
    "# 将word建成index索引的方式，方便后面用embeddingm matrix\n",
    "bow = BOW(data.content.apply(f1).tolist(), min_count=3, maxlen=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-15T12:15:28.760645Z",
     "start_time": "2018-10-15T12:15:28.687428Z"
    }
   },
   "outputs": [],
   "source": [
    "word_embed_dict = {}\n",
    "def get_word_embed_dict():\n",
    "    for i in word2vec.wv.vocab:\n",
    "        word_embed_dict[i] = word2vec.wv.get_vector(i).tolist()\n",
    "#     word_embed_dict['UNK'] = [0]*300\n",
    "    return word_embed_dict\n",
    "word_embed_dict = get_word_embed_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-15T12:15:29.378301Z",
     "start_time": "2018-10-15T12:15:29.322249Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab_size = len(word_embed_dict)\n",
    "embedding_matrix = np.zeros((vocab_size+1,300))\n",
    "\n",
    "for key, value in bow.word2idx.items():\n",
    "    embedding_matrix[value] = word_embed_dict.get(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-15T12:15:29.930849Z",
     "start_time": "2018-10-15T12:15:29.912989Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "       [ 1.17498301e-01,  4.33762133e-01, -2.32221127e-01, ...,\n",
       "        -3.37091684e-01, -2.42774144e-01, -6.83337003e-02],\n",
       "       [ 7.20790505e-01,  1.36479223e+00, -5.62304854e-01, ...,\n",
       "        -7.32061684e-01,  1.20418325e-01, -2.84205794e-01],\n",
       "       ...,\n",
       "       [ 2.44777999e-03,  2.71027833e-02, -1.48612587e-02, ...,\n",
       "        -2.33698953e-02, -1.55870700e-02, -6.94512716e-03],\n",
       "       [ 3.41599551e-03,  2.16386374e-02, -6.68662181e-03, ...,\n",
       "        -1.76747274e-02, -8.45736265e-03, -7.47158891e-03],\n",
       "       [ 3.34755937e-03,  1.41907008e-02, -1.84325725e-02, ...,\n",
       "        -1.95526294e-02, -1.10311620e-02, -1.24784117e-03]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-15T12:56:35.081377Z",
     "start_time": "2018-10-15T12:56:35.070324Z"
    }
   },
   "outputs": [],
   "source": [
    "np.save('save/embedding_matrix',arr=embedding_matrix)\n",
    "# a = np.load('save/embedding_matrix.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-15T12:15:34.585011Z",
     "start_time": "2018-10-15T12:15:34.577761Z"
    }
   },
   "outputs": [],
   "source": [
    "# word对应的index\n",
    "df_train_ = bow.doc2num[train_idx]\n",
    "df_val_ = bow.doc2num[val_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建LSTM模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-16T02:35:18.996896Z",
     "start_time": "2018-10-16T02:35:18.983656Z"
    }
   },
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    '''\n",
    "    并不是所有的配置都生效,实际运行中只根据需求获取自己需要的参数\n",
    "    '''\n",
    "\n",
    "    loss = 'multilabelloss'\n",
    "    model='LSTMText' \n",
    "    num_classes = 30 # 类别\n",
    "    embedding_dim = 300 # embedding大小\n",
    "    linear_hidden_size = 1000 # 全连接层隐藏元数目\n",
    "    kmax_pooling = 2 # k\n",
    "    hidden_size = 128 #LSTM hidden size\n",
    "    num_layers=2 #LSTM layers\n",
    "    inception_dim = 256 #inception的卷积核数\n",
    "    \n",
    "    # vocab_size = 11973 # num of chars\n",
    "    vocab_size = 6894 # num of words \n",
    "    content_seq_len = 100 #描述长度 word为100 char为200\n",
    "    static = False\n",
    "    embedding_path = 'save/embedding_matrix.npy'\n",
    "\n",
    "opt = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-16T02:35:19.750470Z",
     "start_time": "2018-10-16T02:35:19.700539Z"
    }
   },
   "outputs": [],
   "source": [
    "# 相当于把seq_len压缩成k个'词'\n",
    "# dim共三个维度，这里取2即seq_len那个维度，100->k\n",
    "def kmax_pooling(x, dim, k):\n",
    "    index = x.topk(k, dim = dim)[1].sort(dim = dim)[0]\n",
    "    return x.gather(dim, index)\n",
    "\n",
    "class LSTMText(BasicModule): \n",
    "    def __init__(self, opt):\n",
    "        super(LSTMText, self).__init__()\n",
    "        self.model_name = 'LSTMText'\n",
    "        self.opt=opt\n",
    "\n",
    "        kernel_size = self.opt.kernel_size\n",
    "        self.encoder = torch.nn.Embedding(self.opt.vocab_size,self.opt.embedding_dim)\n",
    "\n",
    "        self.content_lstm =torch.nn.LSTM(input_size = self.opt.embedding_dim,\\\n",
    "                            hidden_size = self.opt.hidden_size,\n",
    "                            num_layers = self.opt.num_layers,\n",
    "                            bias = True,\n",
    "                            batch_first = False,\n",
    "                            dropout = 0.5, # dropout\n",
    "                            bidirectional = True\n",
    "                            )\n",
    "\n",
    "        self.fc = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.opt.kmax_pooling*(self.opt.hidden_size*2),self.opt.linear_hidden_size),\n",
    "            torch.nn.Dropout(0.2), # dropout\n",
    "            torch.nn.BatchNorm1d(self.opt.linear_hidden_size),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Linear(self.opt.linear_hidden_size, self.opt.num_classes),\n",
    "            torch.nn.Softmax()\n",
    "        )\n",
    "\n",
    "        if self.opt.embedding_path:\n",
    "            self.encoder.weight.data.copy_(t.from_numpy(np.load(self.opt.embedding_path)))\n",
    " \n",
    "    def forward(self, content):\n",
    "        content = self.encoder(content)\n",
    "        # torch.Size([64, 100, 150])\n",
    "        if self.opt.static:\n",
    "            title=title.detach()\n",
    "            content=content.detach()\n",
    "        \n",
    "        '''\n",
    "        lstm输入的时候需要转成(seq_len, batch, embedding_dim）这种维度（用permute转）<br>\n",
    "        output，每个时刻的LSTM网络的最后一层的输出，维度（seq_len, batch, hidden_size * num_directions）|双向lstm所以输出的hidden_size维度要乘以2<br>\n",
    "        lstm的输出为output, (hn, cn) 的元组<br>\n",
    "        这里取第一个就是output(100,64,256)，第二个是元组其中的第一个hn就是最后时刻的隐层状态hn(4,64,128)\n",
    "        这里的4就是(2层num_layers*双向)lstm得到\n",
    "        '''\n",
    "        content_out = self.content_lstm(content.permute(1,0,2))[0].permute(1,2,0)\n",
    "        #torch.Size([64, 256, 100])\n",
    "        content_conv_out = kmax_pooling((content_out),2,self.opt.kmax_pooling)\n",
    "        conv_out = content_conv_out\n",
    "        reshaped = conv_out.view(conv_out.size(0), -1)\n",
    "        softmax = self.fc((reshaped))\n",
    "        return softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 开始跑模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-16T02:35:21.524756Z",
     "start_time": "2018-10-16T02:35:21.481254Z"
    }
   },
   "outputs": [],
   "source": [
    "EPOCH = 8# 训练整批数据多少次,  我训练了5次\n",
    "BATCH_SIZE = 64\n",
    "LR = 0.002         # 学习率\n",
    "\n",
    "m = LSTMText(opt)\n",
    "if torch.cuda.is_available():\n",
    "    m.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-16T02:35:25.730132Z",
     "start_time": "2018-10-16T02:35:25.719742Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrossEntropyLoss()"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 数据处理成tensor\n",
    "label_tensor = torch.from_numpy(np.array(df_train.subject_1)).long()\n",
    "content_tensor = torch.from_numpy(np.array(df_train_)).long()\n",
    "#val\n",
    "content_val_tensor = torch.from_numpy(np.array(df_val_)).long()\n",
    "label_val_tensor = torch.from_numpy(np.array(df_val.subject_1)).long()\n",
    "\n",
    "torch_dataset = Data.TensorDataset(content_tensor, label_tensor)\n",
    "train_loader = Data.DataLoader(\n",
    "        dataset=torch_dataset,      # torch TensorDataset format\n",
    "        batch_size=BATCH_SIZE,      # mini batch size\n",
    "        shuffle=True,               # random shuffle for training\n",
    "        num_workers=8,              # subprocesses for loading data\n",
    "    )\n",
    "\n",
    "# optimizer, loss_func\n",
    "optimizer = torch.optim.Adam(m.parameters(), lr=LR)   # optimize all lstm parameters;Adam比较好用\n",
    "loss_func = torch.nn.CrossEntropyLoss()   # the target label is not one-hotted 适用于多分类\n",
    "loss_func.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_val_tensor = content_val_tensor.cuda()\n",
    "label_val_tensor = label_val_tensor.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-10-16T02:35:27.518Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=8), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.6/site-packages/torch/nn/modules/container.py:91: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss:  3.1673879623413086\n",
      "val loss:  3.18984317779541\n",
      "training loss:  3.2834389209747314\n",
      "val loss:  3.1693873405456543\n",
      "training loss:  3.0643393993377686\n",
      "val loss:  3.112532615661621\n",
      "training loss:  3.0616390705108643\n",
      "val loss:  3.0707297325134277\n",
      "training loss:  3.021479845046997\n",
      "val loss:  3.042405128479004\n",
      "training loss:  3.0143232345581055\n",
      "val loss:  3.033583402633667\n",
      "training loss:  2.9115610122680664\n",
      "val loss:  3.0212764739990234\n",
      "training loss:  3.135138511657715\n",
      "val loss:  3.025049924850464\n",
      "training loss:  3.040111780166626\n",
      "val loss:  3.0259411334991455\n",
      "training loss:  3.022106647491455\n",
      "val loss:  3.019199848175049\n",
      "training loss:  3.107740879058838\n",
      "val loss:  3.0234880447387695\n",
      "training loss:  2.9572179317474365\n",
      "val loss:  3.0143158435821533\n",
      "training loss:  2.910447120666504\n",
      "val loss:  3.0149362087249756\n",
      "training loss:  3.0686566829681396\n",
      "val loss:  3.008749485015869\n",
      "training loss:  2.9876296520233154\n",
      "val loss:  3.0189781188964844\n",
      "training loss:  3.0338194370269775\n",
      "val loss:  3.022177219390869\n",
      "training loss:  3.075322151184082\n",
      "val loss:  3.000844717025757\n",
      "training loss:  2.9893178939819336\n",
      "val loss:  3.0052897930145264\n",
      "training loss:  2.8767237663269043\n",
      "val loss:  2.9983580112457275\n",
      "training loss:  3.0349481105804443\n",
      "val loss:  2.9990222454071045\n",
      "\n"
     ]
    }
   ],
   "source": [
    "it = 1\n",
    "for epoch in tqdm_notebook(range(EPOCH)):\n",
    "    for step, (content, b_y) in enumerate(train_loader):   # 分配 batch data, normalize x when iterate train_loader\n",
    "        content, b_y = content.cuda(), b_y.cuda()\n",
    "        output = m(content)\n",
    "        loss = loss_func(output, b_y)\n",
    "        if it % 50 == 0:\n",
    "            print('training loss: ', loss.cpu().data.numpy().tolist())\n",
    "            print('val loss: ', loss_func(m(content_val_tensor), label_val_tensor).cpu().data.numpy().tolist())\n",
    "        optimizer.zero_grad()           # clear gradients for this training step\n",
    "        loss.backward()                 # backpropagation, compute gradients\n",
    "        optimizer.step()                # apply gradients\n",
    "        it += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}