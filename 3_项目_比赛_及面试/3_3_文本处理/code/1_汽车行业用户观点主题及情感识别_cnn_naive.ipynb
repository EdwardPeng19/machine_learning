{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as Data\n",
    "import torch as t\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import jieba\n",
    "import gensim\n",
    "from gensim.models import Word2Vec, FastText\n",
    "import re\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "from m import f1_for_car, BOW, BasicModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test_public.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['subject_1'] = data['subject'] + data['sentiment_value'].astype(str)\n",
    "subj_lst = list(filter(lambda x : x is not np.nan, list(set(data.subject_1))))\n",
    "subj_lst_dic = {value:key for key, value in enumerate(subj_lst)}\n",
    "data['subject_1'] = data['subject_1'].apply(lambda x : subj_lst_dic.get(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=5,shuffle=True)\n",
    "for train_idx, val_idx in skf.split(data, data.subject):\n",
    "    break\n",
    "    \n",
    "df_train = data.iloc[train_idx]\n",
    "df_val = data.iloc[val_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 试一下用CNN进行主题分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content_id</th>\n",
       "      <th>content</th>\n",
       "      <th>subject</th>\n",
       "      <th>sentiment_value</th>\n",
       "      <th>sentiment_word</th>\n",
       "      <th>subject_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vUXizsqexyZVRdFH</td>\n",
       "      <td>因为森林人即将换代，这套系统没必要装在一款即将换代的车型上，因为肯定会影响价格。</td>\n",
       "      <td>价格</td>\n",
       "      <td>0</td>\n",
       "      <td>影响</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>QmqJ2AvM5GplaRyz</td>\n",
       "      <td>斯柯达要说质量，似乎比大众要好一点，价格也低一些，用料完全一样。我听说过野帝，但没听说过你说...</td>\n",
       "      <td>价格</td>\n",
       "      <td>1</td>\n",
       "      <td>低</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>KMT1gFJiU4NWrVDn</td>\n",
       "      <td>这玩意都是给有钱任性又不懂车的土豪用的，这价格换一次我妹夫EP020可以换三锅了</td>\n",
       "      <td>价格</td>\n",
       "      <td>-1</td>\n",
       "      <td>有钱任性</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nVIlGd5yMmc37t1o</td>\n",
       "      <td>17价格忒高，估计也就是14-15左右。</td>\n",
       "      <td>价格</td>\n",
       "      <td>-1</td>\n",
       "      <td>高</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>TVciHBPL5XmUxMEd</td>\n",
       "      <td>我开始就是荣放2.5  森林人2.5二选一    荣放主要是底盘质感不行   太硬  其次是...</td>\n",
       "      <td>价格</td>\n",
       "      <td>1</td>\n",
       "      <td>便宜</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         content_id                                            content  \\\n",
       "0  vUXizsqexyZVRdFH           因为森林人即将换代，这套系统没必要装在一款即将换代的车型上，因为肯定会影响价格。   \n",
       "2  QmqJ2AvM5GplaRyz  斯柯达要说质量，似乎比大众要好一点，价格也低一些，用料完全一样。我听说过野帝，但没听说过你说...   \n",
       "3  KMT1gFJiU4NWrVDn           这玩意都是给有钱任性又不懂车的土豪用的，这价格换一次我妹夫EP020可以换三锅了   \n",
       "4  nVIlGd5yMmc37t1o                            17价格忒高，估计也就是14-15左右。      \n",
       "5  TVciHBPL5XmUxMEd  我开始就是荣放2.5  森林人2.5二选一    荣放主要是底盘质感不行   太硬  其次是...   \n",
       "\n",
       "  subject  sentiment_value sentiment_word  subject_1  \n",
       "0      价格                0             影响          1  \n",
       "2      价格                1              低         17  \n",
       "3      价格               -1           有钱任性         12  \n",
       "4      价格               -1              高         12  \n",
       "5      价格                1             便宜         17  "
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载停用词词典\n",
    "with open('data/stop_words_hagongda.txt') as f:\n",
    "    stop_words = f.readlines()\n",
    "    stop_words = [x.replace('\\n','') for x in stop_words]\n",
    "\n",
    "stop_words.append('')\n",
    "    \n",
    "def f1(x):\n",
    "    tmp_l = jieba.cut(x)\n",
    "    return [x.strip() for x in tmp_l if x.strip() not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Word Count: 100%|██████████| 9947/9947 [00:00<00:00, 201934.81it/s]\n",
      "Doc To Number: 100%|██████████| 9947/9947 [00:00<00:00, 137312.46it/s]\n"
     ]
    }
   ],
   "source": [
    "# 将word建成index索引的方式，方便后面用embeddingm matrix\n",
    "bow = BOW(data.content.apply(f1).tolist(), min_count=3, maxlen=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = Word2Vec(data.content.apply(f1).tolist(),size=300,min_count=3)\n",
    "\n",
    "word_embed_dict = {}\n",
    "def get_word_embed_dict():\n",
    "    for i in word2vec.wv.vocab:\n",
    "        word_embed_dict[i] = word2vec.wv.get_vector(i).tolist()\n",
    "#     word_embed_dict['UNK'] = [0]*300\n",
    "    return word_embed_dict\n",
    "word_embed_dict = get_word_embed_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(word_embed_dict)\n",
    "embedding_matrix = np.zeros((vocab_size+1,300))\n",
    "\n",
    "for key, value in bow.word2idx.items():\n",
    "    embedding_matrix[value] = word_embed_dict.get(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "       [ 7.54894853e-01, -6.20306373e-01,  4.13474776e-02, ...,\n",
       "         3.79320860e-01, -7.17586100e-01, -6.60323679e-01],\n",
       "       [ 8.14157128e-01, -2.04117373e-01,  3.04523617e-01, ...,\n",
       "         2.26151064e-01, -6.42750382e-01, -7.18053758e-01],\n",
       "       ...,\n",
       "       [ 1.91839114e-02, -4.97836480e-03,  6.46228669e-03, ...,\n",
       "        -3.93930328e-04, -1.89290550e-02, -1.02663375e-02],\n",
       "       [ 9.69438441e-03, -3.16862273e-03,  1.91619981e-03, ...,\n",
       "        -8.10113677e-04, -1.09908972e-02, -4.49970737e-03],\n",
       "       [ 1.38809122e-02, -3.92513257e-03,  4.00957931e-03, ...,\n",
       "        -1.74620154e-03, -1.59535334e-02, -6.97768107e-03]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('save/embedding_matrix',arr=embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word对应的index\n",
    "df_train_ = bow.doc2num[train_idx]\n",
    "df_val_ = bow.doc2num[val_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN conv1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    '''\n",
    "    并不是所有的配置都生效,实际运行中只根据需求获取自己需要的参数\n",
    "    '''\n",
    "\n",
    "    loss = 'multilabelloss'\n",
    "    model='LSTMText' \n",
    "    title_dim = 100 # 标题的卷积核数\n",
    "    content_dim = 100 #内容的卷积核数\n",
    "    num_classes = 30 # 类别\n",
    "    embedding_dim = 300 # embedding大小\n",
    "    linear_hidden_size = 1000 # 全连接层隐藏元数目\n",
    "    kmax_pooling = 2 # k\n",
    "    hidden_size = 128 #LSTM hidden size\n",
    "    num_layers=2 #LSTM layers\n",
    "    inception_dim = 256 #inception的卷积核数\n",
    "    \n",
    "    kernel_size = 3 #单尺度卷积核\n",
    "    kernel_sizes = [2,3,4] #多尺度卷积核\n",
    "    # vocab_size = 11973 # num of chars\n",
    "    vocab_size = 6597# num of words \n",
    "    content_seq_len = 50 #内容长度 word为50 char为100\n",
    "    static = False\n",
    "    embedding_path = 'save/embedding_matrix.npy'\n",
    "\n",
    "opt = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_sizes =  [1,2,3,4]\n",
    "class MultiCNNTextBNDeep(BasicModule): \n",
    "    def __init__(self, opt ):\n",
    "        super(MultiCNNTextBNDeep, self).__init__()\n",
    "        self.model_name = 'MultiCNNTextBNDeep'\n",
    "        self.opt=opt\n",
    "        self.encoder = nn.Embedding(self.opt.vocab_size,opt.embedding_dim)\n",
    "\n",
    "        content_convs = [nn.Sequential(\n",
    "                                nn.Conv1d(in_channels = self.opt.embedding_dim,\n",
    "                                        out_channels = self.opt.content_dim,\n",
    "                                        kernel_size = kernel_size),\n",
    "                                nn.BatchNorm1d(self.opt.content_dim),\n",
    "                                nn.ReLU(inplace=True),\n",
    "\n",
    "                                nn.Conv1d(in_channels = self.opt.content_dim,\n",
    "                                        out_channels = self.opt.content_dim,\n",
    "                                        kernel_size = kernel_size),\n",
    "                                nn.BatchNorm1d(self.opt.content_dim),\n",
    "                                nn.ReLU(inplace=True),\n",
    "                                # maxpool1d kernel_size=50的意思就是对一句话里每50个单词取maxpool\n",
    "                                nn.MaxPool1d(kernel_size = (self.opt.content_seq_len - kernel_size*2 + 2))\n",
    "                            )\n",
    "            for kernel_size in kernel_sizes]\n",
    "\n",
    "        self.content_convs = nn.ModuleList(content_convs)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(len(kernel_sizes)*self.opt.content_dim,self.opt.linear_hidden_size),\n",
    "            nn.BatchNorm1d(self.opt.linear_hidden_size),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(self.opt.linear_hidden_size,self.opt.num_classes),\n",
    "            nn.Softmax()\n",
    "        )\n",
    "        \n",
    "\n",
    "        if opt.embedding_path:\n",
    "            self.encoder.weight.data.copy_(t.from_numpy(np.load(self.opt.embedding_path)))\n",
    "\n",
    "    def forward(self, content):\n",
    "        content = self.encoder(content)\n",
    "        if self.opt.static:\n",
    "            content.detach()\n",
    "        \n",
    "        \n",
    "        content_out = [content_conv(content.permute(0,2,1)) for content_conv in self.content_convs]\n",
    "#         conv_out = t.cat((title_out+content_out),dim=1)\n",
    "        # t.cat是对list进行拼接，这里对维度1进行拼接\n",
    "        conv_out = t.cat(content_out,dim=1)\n",
    "        reshaped = conv_out.view(conv_out.size(0), -1)\n",
    "        softmax = self.fc((reshaped))\n",
    "        return softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 8           # 训练整批数据多少次,  我只训练了3次\n",
    "BATCH_SIZE = 64\n",
    "LR = 0.002         # 学习率\n",
    "\n",
    "m = MultiCNNTextBNDeep(opt)\n",
    "if torch.cuda.is_available():\n",
    "    m.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrossEntropyLoss()"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 数据处理成tensor\n",
    "label_tensor = torch.from_numpy(np.array(df_train.subject_1)).long()\n",
    "content_tensor = torch.from_numpy(np.array(df_train_)).long()\n",
    "#val\n",
    "content_val_tensor = torch.from_numpy(np.array(df_val_)).long()\n",
    "label_val_tensor = torch.from_numpy(np.array(df_val.subject_1)).long()\n",
    "\n",
    "torch_dataset = Data.TensorDataset(content_tensor, label_tensor)\n",
    "train_loader = Data.DataLoader(\n",
    "        dataset=torch_dataset,      # torch TensorDataset format\n",
    "        batch_size=BATCH_SIZE,      # mini batch size\n",
    "        shuffle=True,               # random shuffle for training\n",
    "        num_workers=8,              # subprocesses for loading data\n",
    "    )\n",
    "\n",
    "# optimizer, loss_func\n",
    "optimizer = torch.optim.Adam(m.parameters(), lr=LR)   # optimize all lstm parameters;Adam比较好用\n",
    "loss_func = torch.nn.CrossEntropyLoss()   # the target label is not one-hotted 适用于多分类\n",
    "loss_func.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_val_tensor = content_val_tensor.cuda()\n",
    "label_val_tensor = label_val_tensor.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a0a648c383541e5b6c5fe1e27aa6e30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=8), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.6/site-packages/torch/nn/modules/container.py:91: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss:  3.111666202545166\n",
      "val loss:  3.082869291305542\n",
      "training loss:  3.124356269836426\n",
      "val loss:  3.030306577682495\n",
      "training loss:  3.0201728343963623\n",
      "val loss:  3.0152130126953125\n",
      "training loss:  2.919506549835205\n",
      "val loss:  3.0144176483154297\n",
      "training loss:  3.066793918609619\n",
      "val loss:  2.999502420425415\n",
      "training loss:  3.004420757293701\n",
      "val loss:  2.9991745948791504\n",
      "training loss:  3.004478693008423\n",
      "val loss:  3.0036096572875977\n",
      "training loss:  2.9099767208099365\n",
      "val loss:  2.99699330329895\n",
      "training loss:  3.009580612182617\n",
      "val loss:  2.992452383041382\n",
      "training loss:  3.1874115467071533\n",
      "val loss:  3.0018603801727295\n",
      "training loss:  3.0316457748413086\n",
      "val loss:  2.9846084117889404\n",
      "training loss:  2.975032329559326\n",
      "val loss:  2.993483066558838\n",
      "training loss:  2.8965225219726562\n",
      "val loss:  2.9855575561523438\n",
      "training loss:  3.0033328533172607\n",
      "val loss:  2.975074052810669\n",
      "training loss:  2.8534862995147705\n",
      "val loss:  2.9710640907287598\n",
      "training loss:  2.9624392986297607\n",
      "val loss:  2.963073968887329\n",
      "training loss:  2.8668296337127686\n",
      "val loss:  2.966496467590332\n",
      "training loss:  2.9743380546569824\n",
      "val loss:  2.9701762199401855\n",
      "training loss:  2.876049041748047\n",
      "val loss:  2.9676201343536377\n",
      "training loss:  2.71454119682312\n",
      "val loss:  2.9716544151306152\n",
      "\n"
     ]
    }
   ],
   "source": [
    "it = 1\n",
    "for epoch in tqdm_notebook(range(EPOCH)):\n",
    "    for step, (content, b_y) in enumerate(train_loader):   # 分配 batch data, normalize x when iterate train_loader\n",
    "        content, b_y = content.cuda(), b_y.cuda()\n",
    "        output = m(content)\n",
    "        loss = loss_func(output, b_y)\n",
    "        if it % 50 == 0:\n",
    "            print('training loss: ', loss.cpu().data.numpy().tolist())\n",
    "            print('val loss: ', loss_func(m(content_val_tensor), label_val_tensor).cpu().data.numpy().tolist())\n",
    "        optimizer.zero_grad()           # clear gradients for this training step\n",
    "        loss.backward()                 # backpropagation, compute gradients\n",
    "        optimizer.step()                # apply gradients\n",
    "        it += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
